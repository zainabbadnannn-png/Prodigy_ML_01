# -*- coding: utf-8 -*-
"""Untitled28.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xrrlqb6XCyynGaUxSRIsV0OKNUOFZMH4
"""

# ============================================================================
# HOUSE PRICE PREDICTION - LINEAR REGRESSION (Google Colab Version - FIXED)
# ============================================================================

# Install required packages
!pip install -q pandas numpy matplotlib seaborn scikit-learn

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

print("âœ… All packages imported successfully!")

# ============================================================================
# STEP 1: CREATE SYNTHETIC DATASET (SIMPLIFIED)
# ============================================================================

print("\nğŸ“Š CREATING SYNTHETIC HOUSING DATASET...")
print("=" * 50)

# Create synthetic housing data
np.random.seed(42)
n_samples = 1460

print(f"Creating {n_samples} synthetic house samples...")

# Generate basic features
living_area = np.random.normal(1500, 500, n_samples).clip(500, 4000)
bedrooms = np.random.choice([2, 3, 4, 5], n_samples, p=[0.3, 0.4, 0.2, 0.1])
full_bath = np.random.choice([1, 2, 3], n_samples, p=[0.4, 0.5, 0.1])
half_bath = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])

# Calculate price based on features with some noise
base_price = 50000
price = (base_price +
         living_area * 150 +
         bedrooms * 30000 +
         full_bath * 20000 +
         half_bath * 10000 +
         np.random.normal(0, 30000, n_samples))

# Create DataFrame with just the essential features we need
df = pd.DataFrame({
    'Id': range(1, n_samples + 1),
    'GrLivArea': living_area,
    'BedroomAbvGr': bedrooms,
    'FullBath': full_bath,
    'HalfBath': half_bath,
    'OverallQual': np.random.choice(range(1, 11), n_samples),
    'YearBuilt': np.random.randint(1950, 2020, n_samples),
    'GarageCars': np.random.choice([0, 1, 2, 3], n_samples, p=[0.1, 0.3, 0.5, 0.1]),
    'SalePrice': price
})

print("âœ… Synthetic dataset created successfully!")
print(f"\nDataset shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")

# Display first few rows
print("\nFirst 5 rows:")
display(df.head())

# ============================================================================
# STEP 2: EXPLORE THE DATASET
# ============================================================================

print("\nğŸ” EXPLORING THE DATASET...")
print("=" * 50)

# Display basic information
print(f"Total samples: {df.shape[0]}")
print(f"Total features: {df.shape[1]}")

# Basic statistics
print("\nğŸ“Š BASIC STATISTICS:")
print(df.describe())

# Check for missing values
print("\nğŸ” MISSING VALUES:")
missing_values = df.isnull().sum()
if missing_values.sum() == 0:
    print("No missing values in the dataset!")
else:
    print(missing_values[missing_values > 0])

# ============================================================================
# STEP 3: PREPARE FEATURES
# ============================================================================

print("\nğŸ”§ PREPARING FEATURES...")
print("=" * 50)

# Create total bathrooms feature
df['TotalBath'] = df['FullBath'] + 0.5 * df['HalfBath']

# Define our features
features = ['GrLivArea', 'BedroomAbvGr', 'TotalBath']
target = 'SalePrice'

print("Selected features for the model:")
print("1. GrLivArea - Above grade living area (square footage)")
print("2. BedroomAbvGr - Number of bedrooms above ground")
print("3. TotalBath - Total bathrooms (FullBath + 0.5*HalfBath)")
print(f"\nTarget variable: {target}")

# Create clean dataset
df_clean = df[features + [target]].copy()

print(f"\nâœ… Features prepared:")
print(f"   Features: {features}")
print(f"   Target: {target}")
print(f"   Samples: {len(df_clean)}")

# ============================================================================
# STEP 4: EXPLORATORY DATA ANALYSIS
# ============================================================================

print("\nğŸ“Š EXPLORATORY DATA ANALYSIS...")
print("=" * 50)

# Set up the visualization style
plt.style.use('default')
sns.set_palette("husl")

# Create subplots
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('House Price Dataset Analysis', fontsize=16, fontweight='bold', y=1.02)

# 1. Distribution of Sale Prices
axes[0, 0].hist(df_clean[target], bins=50, edgecolor='black', alpha=0.7, color='skyblue')
axes[0, 0].set_title('Distribution of Sale Prices', fontsize=12, fontweight='bold')
axes[0, 0].set_xlabel('Sale Price ($)')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].grid(True, alpha=0.3)

# 2. Living Area vs Price
scatter = axes[0, 1].scatter(df_clean['GrLivArea'], df_clean[target], alpha=0.6, c='green', s=30)
axes[0, 1].set_title('Living Area vs Sale Price', fontsize=12, fontweight='bold')
axes[0, 1].set_xlabel('Living Area (sq ft)')
axes[0, 1].set_ylabel('Sale Price ($)')
axes[0, 1].grid(True, alpha=0.3)

# 3. Bedrooms vs Average Price
bedroom_avg = df_clean.groupby('BedroomAbvGr')[target].mean()
bars = axes[0, 2].bar(bedroom_avg.index, bedroom_avg.values, color='lightcoral')
axes[0, 2].set_title('Average Price by Bedrooms', fontsize=12, fontweight='bold')
axes[0, 2].set_xlabel('Number of Bedrooms')
axes[0, 2].set_ylabel('Average Sale Price ($)')
axes[0, 2].grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    axes[0, 2].text(bar.get_x() + bar.get_width()/2., height,
                   f'${height:,.0f}', ha='center', va='bottom', fontsize=9)

# 4. Bathrooms vs Price
bath_avg = df_clean.groupby('TotalBath')[target].mean()
axes[1, 0].bar(bath_avg.index, bath_avg.values, color='orange')
axes[1, 0].set_title('Average Price by Bathrooms', fontsize=12, fontweight='bold')
axes[1, 0].set_xlabel('Number of Bathrooms')
axes[1, 0].set_ylabel('Average Sale Price ($)')
axes[1, 0].grid(True, alpha=0.3, axis='y')

# 5. Correlation Heatmap
correlation = df_clean.corr()
sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1, 1], center=0)
axes[1, 1].set_title('Feature Correlation Matrix', fontsize=12, fontweight='bold')

# 6. Box plot of features
box_data = [df_clean['GrLivArea'], df_clean['BedroomAbvGr'], df_clean['TotalBath'], df_clean[target]]
box_labels = ['Living Area', 'Bedrooms', 'Bathrooms', 'Sale Price']
axes[1, 2].boxplot(box_data, labels=box_labels)
axes[1, 2].set_title('Feature Distributions', fontsize=12, fontweight='bold')
axes[1, 2].set_ylabel('Values')
axes[1, 2].tick_params(axis='x', rotation=45)
axes[1, 2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Display correlation values
print("\nğŸ“ˆ FEATURE CORRELATIONS WITH SALE PRICE:")
for feature in features:
    corr = df_clean[feature].corr(df_clean[target])
    print(f"  {feature}: {corr:.3f}")

# ============================================================================
# STEP 5: FEATURE ENGINEERING
# ============================================================================

print("\nâš™ï¸ FEATURE ENGINEERING...")
print("=" * 50)

# Check for outliers in living area
Q1 = df_clean['GrLivArea'].quantile(0.25)
Q3 = df_clean['GrLivArea'].quantile(0.75)
IQR = Q3 - Q1
upper_bound = Q3 + 1.5 * IQR
lower_bound = Q1 - 1.5 * IQR

outliers = df_clean[(df_clean['GrLivArea'] > upper_bound) | (df_clean['GrLivArea'] < lower_bound)]
print(f"Outliers in Living Area: {len(outliers)} ({len(outliers)/len(df_clean)*100:.1f}%)")

# Remove outliers
df_clean = df_clean[(df_clean['GrLivArea'] <= upper_bound) & (df_clean['GrLivArea'] >= lower_bound)]
print(f"Samples after removing outliers: {len(df_clean)}")

# Create interaction features
df_clean['AreaPerBedroom'] = df_clean['GrLivArea'] / (df_clean['BedroomAbvGr'].replace(0, 1))
df_clean['BedBathRatio'] = df_clean['BedroomAbvGr'] / (df_clean['TotalBath'].replace(0, 0.1))

# Update features list
features_extended = ['GrLivArea', 'BedroomAbvGr', 'TotalBath', 'AreaPerBedroom', 'BedBathRatio']

print(f"\nâœ… Feature engineering completed:")
print(f"   Created 'AreaPerBedroom': Living area per bedroom")
print(f"   Created 'BedBathRatio': Bedroom to bathroom ratio")
print(f"   Total features: {len(features_extended)}")

# ============================================================================
# STEP 6: DATA PREPROCESSING
# ============================================================================

print("\nğŸ”„ DATA PREPROCESSING...")
print("=" * 50)

# Prepare features and target
X = df_clean[features_extended]
y = df_clean[target]

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"\nData split:")
print(f"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"  Testing set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)")

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nâœ… Feature scaling completed (StandardScaler)")

# ============================================================================
# STEP 7: MODEL TRAINING
# ============================================================================

print("\nğŸ¤– TRAINING LINEAR REGRESSION MODEL...")
print("=" * 50)

# Create and train the model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

print("âœ… Model training completed!")

# Display model coefficients
coefficients = pd.DataFrame({
    'Feature': features_extended,
    'Coefficient': model.coef_,
    'Absolute_Impact': abs(model.coef_)
}).sort_values('Absolute_Impact', ascending=False)

print("\nğŸ“Š MODEL COEFFICIENTS:")
print("=" * 40)
print(coefficients.to_string(index=False))

print(f"\nModel Intercept: ${model.intercept_:,.2f}")

# Visualize feature importance
plt.figure(figsize=(10, 6))
bars = plt.barh(coefficients['Feature'], coefficients['Absolute_Impact'], color='teal')
plt.xlabel('Absolute Coefficient Value', fontsize=12)
plt.title('Feature Importance in Linear Regression Model', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# Add value labels
for bar in bars:
    width = bar.get_width()
    plt.text(width + max(coefficients['Absolute_Impact']) * 0.01, bar.get_y() + bar.get_height()/2,
            f'{width:,.2f}', va='center', fontsize=10)

plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

# ============================================================================
# STEP 8: MODEL EVALUATION
# ============================================================================

print("\nğŸ“ˆ MODEL EVALUATION...")
print("=" * 50)

# Make predictions
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)

# Calculate metrics
def calculate_metrics(y_true, y_pred, set_name):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    # Calculate percentage errors
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

    return {
        'Set': set_name,
        'RMSE': rmse,
        'MAE': mae,
        'RÂ²': r2,
        'MAPE (%)': mape
    }

# Calculate metrics for both sets
train_metrics = calculate_metrics(y_train, y_train_pred, 'Training')
test_metrics = calculate_metrics(y_test, y_test_pred, 'Testing')

# Create metrics dataframe
metrics_df = pd.DataFrame([train_metrics, test_metrics])

print("\nğŸ“Š PERFORMANCE METRICS:")
print("=" * 60)
print(metrics_df.to_string(index=False))

print("\nğŸ¯ INTERPRETATION:")
print(f"â€¢ RÂ² Score: Model explains {test_metrics['RÂ²']*100:.1f}% of price variance")
print(f"â€¢ Average Error: Â±${test_metrics['MAE']:,.0f}")
print(f"â€¢ Percentage Error: {test_metrics['MAPE (%)']:.1f}% on average")

# ============================================================================
# STEP 9: VISUALIZE RESULTS
# ============================================================================

print("\nğŸ“Š VISUALIZING RESULTS...")
print("=" * 50)

# Create visualizations
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Model Performance Analysis', fontsize=16, fontweight='bold', y=1.02)

# 1. Actual vs Predicted
axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6, color='royalblue')
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', lw=2, label='Perfect Prediction')
axes[0, 0].set_xlabel('Actual Prices ($)', fontsize=12)
axes[0, 0].set_ylabel('Predicted Prices ($)', fontsize=12)
axes[0, 0].set_title('Actual vs Predicted Prices', fontsize=14, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. Residual plot
residuals = y_test - y_test_pred
axes[0, 1].scatter(y_test_pred, residuals, alpha=0.6, color='purple')
axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[0, 1].set_xlabel('Predicted Prices ($)', fontsize=12)
axes[0, 1].set_ylabel('Residuals ($)', fontsize=12)
axes[0, 1].set_title('Residual Plot', fontsize=14, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3)

# 3. Error metrics comparison
metrics_names = ['RMSE', 'MAE']
train_values = [train_metrics['RMSE'], train_metrics['MAE']]
test_values = [test_metrics['RMSE'], test_metrics['MAE']]

x = np.arange(len(metrics_names))
width = 0.35

axes[1, 0].bar(x - width/2, train_values, width, label='Training', color='lightblue')
axes[1, 0].bar(x + width/2, test_values, width, label='Testing', color='lightcoral')

axes[1, 0].set_xlabel('Metrics', fontsize=12)
axes[1, 0].set_ylabel('Error ($)', fontsize=12)
axes[1, 0].set_title('Error Metrics Comparison', fontsize=14, fontweight='bold')
axes[1, 0].set_xticks(x)
axes[1, 0].set_xticklabels(metrics_names)
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3, axis='y')

# Add value labels
for i, (train_val, test_val) in enumerate(zip(train_values, test_values)):
    axes[1, 0].text(i - width/2, train_val, f'${train_val:,.0f}',
                   ha='center', va='bottom', fontsize=10)
    axes[1, 0].text(i + width/2, test_val, f'${test_val:,.0f}',
                   ha='center', va='bottom', fontsize=10)

# 4. RÂ² Score comparison
r2_data = [train_metrics['RÂ²'], test_metrics['RÂ²']]
bars = axes[1, 1].bar(['Training', 'Testing'], r2_data, color=['blue', 'orange'])
axes[1, 1].set_ylabel('RÂ² Score', fontsize=12)
axes[1, 1].set_title('Model Fit (RÂ² Score)', fontsize=14, fontweight='bold')
axes[1, 1].set_ylim([0, 1])
axes[1, 1].grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for bar, r2 in zip(bars, r2_data):
    height = bar.get_height()
    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height - 0.05,
                   f'{r2:.3f}', ha='center', va='bottom', fontsize=12, color='white',
                   fontweight='bold')

plt.tight_layout()
plt.show()

# ============================================================================
# STEP 10: MAKE PREDICTIONS
# ============================================================================

print("\nğŸ  MAKING EXAMPLE PREDICTIONS...")
print("=" * 50)

# Create example houses
example_houses = pd.DataFrame({
    'GrLivArea': [1200, 1800, 2400, 3000],
    'BedroomAbvGr': [2, 3, 4, 5],
    'TotalBath': [1.5, 2.0, 2.5, 3.0],
    'AreaPerBedroom': [1200/3, 1800/4, 2400/5, 3000/6],
    'BedBathRatio': [2/1.6, 3/2.1, 4/2.6, 5/3.1]
})

# Ensure all features are present
for feature in features_extended:
    if feature not in example_houses.columns:
        # Fill missing with median
        example_houses[feature] = df_clean[feature].median()

# Scale and predict
example_scaled = scaler.transform(example_houses[features_extended])
example_predictions = model.predict(example_scaled)

print("\nğŸ“ EXAMPLE HOUSE PRICE PREDICTIONS:")
print("=" * 60)

for i, (idx, row) in enumerate(example_houses.iterrows()):
    print(f"\nğŸ¡ House {i+1}:")
    print(f"  Living Area: {int(row['GrLivArea'])} sq ft")
    print(f"  Bedrooms: {int(row['BedroomAbvGr'])}")
    print(f"  Bathrooms: {row['TotalBath']:.1f}")
    print(f"  Area per Bedroom: {row['AreaPerBedroom']:.0f} sq ft")
    print(f"  Bedroom to Bathroom Ratio: {row['BedBathRatio']:.1f}")
    print(f"  {'â”€' * 30}")
    print(f"  ğŸ“Š PREDICTED PRICE: ${example_predictions[i]:,.2f}")
    print(f"  {'â•' * 30}")

# Visualize example predictions
plt.figure(figsize=(10, 6))
x_pos = np.arange(len(example_predictions))
colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightsalmon']
bars = plt.bar(x_pos, example_predictions, color=colors)

plt.xlabel('House Example', fontsize=12)
plt.ylabel('Predicted Price ($)', fontsize=12)
plt.title('Predicted Prices for Example Houses', fontsize=14, fontweight='bold')
plt.xticks(x_pos, ['Small Home', 'Family Home', 'Large Home', 'Luxury Home'])

# Add price labels on bars
for bar, price in zip(bars, example_predictions):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
            f'${price:,.0f}', ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

# ============================================================================
# STEP 11: SAVE THE MODEL
# ============================================================================

print("\nğŸ’¾ SAVING THE MODEL...")
print("=" * 50)

import joblib
from datetime import datetime

# Create timestamp
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Save model and scaler
model_filename = f'house_price_model_{timestamp}.pkl'
scaler_filename = f'scaler_{timestamp}.pkl'

joblib.dump(model, model_filename)
joblib.dump(scaler, scaler_filename)

print(f"âœ… Model saved as: {model_filename}")
print(f"âœ… Scaler saved as: {scaler_filename}")

# Test loading
loaded_model = joblib.load(model_filename)
loaded_scaler = joblib.load(scaler_filename)

test_pred = loaded_model.predict(loaded_scaler.transform(example_houses[features_extended].iloc[[0]]))
print(f"\nğŸ§ª Test prediction with loaded model:")
print(f"   First example house: ${test_pred[0]:,.2f}")

# Save to Google Drive (optional)
try:
    from google.colab import drive
    drive.mount('/content/drive')

    drive_path = '/content/drive/MyDrive/house_price_prediction/'
    import os
    os.makedirs(drive_path, exist_ok=True)

    drive_model_path = os.path.join(drive_path, model_filename)
    drive_scaler_path = os.path.join(drive_path, scaler_filename)

    joblib.dump(model, drive_model_path)
    joblib.dump(scaler, drive_scaler_path)

    print(f"\nğŸ’¾ Model also saved to Google Drive:")
    print(f"   {drive_model_path}")
    print(f"   {drive_scaler_path}")
except:
    print("\nâš ï¸ Google Drive not mounted. Files saved locally only.")

# ============================================================================
# STEP 12: SUMMARY
# ============================================================================

print("\n" + "=" * 60)
print("ğŸ¯ PROJECT SUMMARY")
print("=" * 60)

print(f"\nğŸ“Š MODEL PERFORMANCE:")
print(f"  â€¢ RÂ² Score: {test_metrics['RÂ²']:.3f} ({test_metrics['RÂ²']*100:.1f}% variance explained)")
print(f"  â€¢ Average Error: Â±${test_metrics['MAE']:,.0f}")
print(f"  â€¢ RMSE: ${test_metrics['RMSE']:,.0f}")
print(f"  â€¢ Accuracy: {100 - test_metrics['MAPE (%)']:.1f}%")

print(f"\nğŸ”‘ KEY INSIGHTS:")
print(f"  1. Most important feature: {coefficients.iloc[0]['Feature']}")
print(f"  2. Number of features used: {len(features_extended)}")
print(f"  3. Dataset size: {len(df_clean)} samples")

print(f"\nğŸ“ˆ PRICE PREDICTION FORMULA:")
print(f"  Price = ${model.intercept_:,.0f}", end="")
for i, (feature, coef) in enumerate(zip(features_extended, model.coef_)):
    sign = " + " if coef >= 0 else " - "
    print(f"{sign} ({abs(coef):.1f} Ã— {feature})", end="")
print("\n  (All features are standardized)")

print(f"\nâœ… PROJECT COMPLETED SUCCESSFULLY!")

# Display final summary
from IPython.display import display, HTML

summary_html = f"""
<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 25px; border-radius: 15px; color: white; margin-top: 20px;">
    <h2 style="text-align: center; margin-bottom: 20px;">ğŸ† House Price Prediction Results</h2>

    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin-bottom: 20px;">
        <div style="background: rgba(255, 255, 255, 0.1); padding: 15px; border-radius: 10px; backdrop-filter: blur(10px);">
            <h3 style="margin-top: 0;">ğŸ“ˆ Model Accuracy</h3>
            <h1 style="font-size: 2.5em; margin: 10px 0;">{100 - test_metrics['MAPE (%)']:.1f}%</h1>
            <p>Prediction Accuracy</p>
        </div>

        <div style="background: rgba(255, 255, 255, 0.1); padding: 15px; border-radius: 10px; backdrop-filter: blur(10px);">
            <h3 style="margin-top: 0;">ğŸ’° Avg Error</h3>
            <h1 style="font-size: 2.5em; margin: 10px 0;">${test_metrics['MAE']:,.0f}</h1>
            <p>Mean Absolute Error</p>
        </div>

        <div style="background: rgba(255, 255, 255, 0.1); padding: 15px; border-radius: 10px; backdrop-filter: blur(10px);">
            <h3 style="margin-top: 0;">ğŸ¯ RÂ² Score</h3>
            <h1 style="font-size: 2.5em; margin: 10px 0;">{test_metrics['RÂ²']:.3f}</h1>
            <p>Variance Explained</p>
        </div>

        <div style="background: rgba(255, 255, 255, 0.1); padding: 15px; border-radius: 10px; backdrop-filter: blur(10px);">
            <h3 style="margin-top: 0;">ğŸ  Features</h3>
            <h1 style="font-size: 2.5em; margin: 10px 0;">{len(features_extended)}</h1>
            <p>Predictor Variables</p>
        </div>
    </div>

    <div style="text-align: center; margin-top: 20px;">
        <p style="font-size: 1.1em; opacity: 0.9;">
            Linear regression model trained on {len(df_clean)} house samples.<br>
            Model saved as <code>{model_filename}</code>
        </p>
    </div>
</div>
"""

display(HTML(summary_html))

print("\n" + "=" * 60)
print("ğŸ ALL STEPS COMPLETED SUCCESSFULLY! ğŸ")
print("=" * 60)